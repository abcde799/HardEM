{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from make_df import create_df, make_inputs, create_nscar_df\n",
    "from naive import naive_fit, get_true_labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid a dataset with only one label we append the following extra columns.\n",
    "\n",
    "extra0 = pd.DataFrame([[0.1,0.1,0.1,0,1,0]],columns = ['x1', 'x2', 'x3', 'cure_label', 'int', 'nscar_censoring_indicator'])\n",
    "\n",
    "extra1 = pd.DataFrame([[0.1,0.1,0.1,1,1,1]],columns = ['x1', 'x2', 'x3', 'cure_label', 'int', 'nscar_censoring_indicator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = pd.concat([extra0, extra1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In these tests we compart our algo with the clustering approach\n",
    "\n",
    "r_weights = np.random.uniform(-0.9,0.9,(10,4))\n",
    "r_nscar_weights = np.random.uniform(-0.9,0.9,(10,4)) #covariate weights determining prob of being censored \n",
    "scores = []\n",
    "covariates = ['x1', 'x2', 'x3']\n",
    "dist = [[0, 1], [0, 1], [0, 1]]\n",
    "cols = ['nscar_censoring_indicator', 'cure_label']\n",
    "\n",
    "for test_model_weights in r_weights: \n",
    "    \n",
    "    for nscar_weights in r_nscar_weights:\n",
    "    \n",
    "        foo = create_nscar_df(covariates, dist, 150, test_model_weights, nscar_weights)\n",
    "    \n",
    "        foo = pd.concat([foo, extra])\n",
    "    \n",
    "        censored_inputs = make_inputs(foo, 0, cols)\n",
    "\n",
    "        noncensored_inputs = make_inputs(foo, 1, cols)\n",
    "    \n",
    "        fit = naive_fit(censored_inputs, noncensored_inputs, 'use_HardEM')    \n",
    "    \n",
    "        y_pred = fit['pred']\n",
    "    \n",
    "        y_true = get_true_labels(foo, ['nscar_censoring_indicator', 'cure_label'])\n",
    "    \n",
    "        y_scores = fit['prob']\n",
    "    \n",
    "        hard_acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "        hard_auc = roc_auc_score(y_true, y_scores)\n",
    "        \n",
    "        hard_logloss = log_loss(y_true, y_scores)\n",
    "    \n",
    "        fit_naive = naive_fit(censored_inputs, noncensored_inputs, 'use_clustering')\n",
    "    \n",
    "        y_pred_naive = fit_naive['pred']\n",
    "    \n",
    "        y_true_naive = get_true_labels(foo, ['nscar_censoring_indicator', 'cure_label'])\n",
    "    \n",
    "        y_scores_naive = fit_naive['prob']\n",
    "    \n",
    "        naive_acc = accuracy_score(y_true_naive, y_pred_naive)\n",
    "    \n",
    "        naive_auc = roc_auc_score(y_true_naive, y_scores_naive)\n",
    "        \n",
    "        naive_logloss = log_loss(y_true_naive, y_scores_naive)\n",
    "    \n",
    "\n",
    "        scores.append([hard_auc, naive_auc, hard_acc, naive_acc, hard_logloss, naive_logloss])\n",
    "    \n",
    "new_df = pd.DataFrame(columns=['Hard_auc', 'Naive_auc', 'Hard_acc', 'Naive_acc', 'Hard_ll', 'Naive_ll'], data=scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[new_df['Hard_ll']-new_df['Naive_ll']<0].index)\n",
    "#How many times our logloss score was lower (better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[new_df['Hard_auc']-new_df['Naive_auc']>0].index)\n",
    "#How many times our AUC score was higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[new_df['Hard_acc']-new_df['Naive_acc']>0].index) \n",
    "#How many times our accuracy was higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In these tests we compart our algo with 'all censored assumed cured'\n",
    "\n",
    "r_weights = np.random.uniform(-0.9,0.9,(10,4))\n",
    "r_nscar_weights = np.random.uniform(-0.9,0.9,(10,4)) #covariate weights determining prob of being censored \n",
    "scores = []\n",
    "covariates = ['x1', 'x2', 'x3']\n",
    "dist = [[0, 1], [0, 1], [0, 1]]\n",
    "cols = ['nscar_censoring_indicator', 'cure_label']\n",
    "\n",
    "for test_model_weights in r_weights: \n",
    "    \n",
    "    for nscar_weights in r_nscar_weights:\n",
    "    \n",
    "        foo = create_nscar_df(covariates, dist, 150, test_model_weights, nscar_weights)\n",
    "    \n",
    "        foo = pd.concat([foo, extra])\n",
    "    \n",
    "        censored_inputs = make_inputs(foo, 0, cols)\n",
    "\n",
    "        noncensored_inputs = make_inputs(foo, 1, cols)\n",
    "    \n",
    "        fit = naive_fit(censored_inputs, noncensored_inputs, 'use_HardEM')    \n",
    "    \n",
    "        y_pred = fit['pred']\n",
    "    \n",
    "        y_true = get_true_labels(foo, ['nscar_censoring_indicator', 'cure_label'])\n",
    "    \n",
    "        y_scores = fit['prob']\n",
    "    \n",
    "        hard_acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "        hard_auc = roc_auc_score(y_true, y_scores)\n",
    "        \n",
    "        hard_logloss = log_loss(y_true, y_scores)\n",
    "    \n",
    "        fit_naive = naive_fit(censored_inputs, noncensored_inputs, 'all_cens_cured')\n",
    "    \n",
    "        y_pred_naive = fit_naive['pred']\n",
    "    \n",
    "        y_true_naive = get_true_labels(foo, ['nscar_censoring_indicator', 'cure_label'])\n",
    "    \n",
    "        y_scores_naive = fit_naive['prob']\n",
    "    \n",
    "        naive_acc = accuracy_score(y_true_naive, y_pred_naive)\n",
    "    \n",
    "        naive_auc = roc_auc_score(y_true_naive, y_scores_naive)\n",
    "        \n",
    "        naive_logloss = log_loss(y_true_naive, y_scores_naive)\n",
    "    \n",
    "\n",
    "        scores.append([hard_auc, naive_auc, hard_acc, naive_acc, hard_logloss, naive_logloss])\n",
    "    \n",
    "new_df = pd.DataFrame(columns=['Hard_auc', 'Naive_auc', 'Hard_acc', 'Naive_acc', 'Hard_ll', 'Naive_ll'], data=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[new_df['Hard_ll']-new_df['Naive_ll']<0].index)\n",
    "#How many times our logloss score was lower (better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[new_df['Hard_auc']-new_df['Naive_auc']>0].index)\n",
    "#How many times our AUC score was higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[new_df['Hard_acc']-new_df['Naive_acc']>0].index) \n",
    "#How many times our accuracy was higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In these tests we compart our algo with the 'fifty-fifty' approach.\n",
    "\n",
    "r_weights = np.random.uniform(-0.9,0.9,(32,4))\n",
    "r_nscar_weights = np.random.uniform(-0.9,0.9,(31,4)) #covariate weights determining prob of being censored \n",
    "scores = []\n",
    "covariates = ['x1', 'x2', 'x3']\n",
    "dist = [[0, 1], [0, 1], [0, 1]]\n",
    "cols = ['nscar_censoring_indicator', 'cure_label']\n",
    "\n",
    "for test_model_weights in r_weights: \n",
    "    \n",
    "    for nscar_weights in r_nscar_weights:\n",
    "    \n",
    "        foo = create_nscar_df(covariates, dist, 150, test_model_weights, nscar_weights)\n",
    "    \n",
    "        foo = pd.concat([foo, extra])\n",
    "    \n",
    "        censored_inputs = make_inputs(foo, 0, cols)\n",
    "\n",
    "        noncensored_inputs = make_inputs(foo, 1, cols)\n",
    "    \n",
    "        fit = naive_fit(censored_inputs, noncensored_inputs, 'use_HardEM')    \n",
    "    \n",
    "        y_pred = fit['pred']\n",
    "    \n",
    "        y_true = get_true_labels(foo, ['nscar_censoring_indicator', 'cure_label'])\n",
    "    \n",
    "        y_scores = fit['prob']\n",
    "    \n",
    "        hard_acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "        hard_auc = roc_auc_score(y_true, y_scores)\n",
    "        \n",
    "        hard_logloss = log_loss(y_true, y_scores)\n",
    "    \n",
    "        fit_naive = naive_fit(censored_inputs, noncensored_inputs, 'fifty_fifty')\n",
    "    \n",
    "        y_pred_naive = fit_naive['pred']\n",
    "    \n",
    "        y_true_naive = get_true_labels(foo, ['nscar_censoring_indicator', 'cure_label'])\n",
    "    \n",
    "        y_scores_naive = fit_naive['prob']\n",
    "    \n",
    "        naive_acc = accuracy_score(y_true_naive, y_pred_naive)\n",
    "    \n",
    "        naive_auc = roc_auc_score(y_true_naive, y_scores_naive)\n",
    "        \n",
    "        naive_logloss = log_loss(y_true_naive, y_scores_naive)\n",
    "    \n",
    "\n",
    "        scores.append([hard_auc, naive_auc, hard_acc, naive_acc, hard_logloss, naive_logloss])\n",
    "    \n",
    "new_df = pd.DataFrame(columns=['Hard_auc', 'Naive_auc', 'Hard_acc', 'Naive_acc', 'Hard_ll', 'Naive_ll'], data=scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[new_df['Hard_ll']-new_df['Naive_ll']<0].index)\n",
    "#How many times our logloss score was lower (better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[new_df['Hard_auc']-new_df['Naive_auc']>0].index)\n",
    "#How many times our AUC score was higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[new_df['Hard_acc']-new_df['Naive_acc']>0].index) \n",
    "#How many times our accuracy was higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
